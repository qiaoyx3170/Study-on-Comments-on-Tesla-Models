{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njNrnddg204_"
   },
   "source": [
    "In this notebook, we extract the comments of Reddit posts and YouTube videos that relate to Tesla Model\n",
    "\n",
    "\n",
    "--To get the comments from Reddit, we use the API praw. We first search the posts with the keyword 'Tesla Model', then extract the comments under these posts.\n",
    "\n",
    "--To get the comments from YouTube, we use the Selenium web scraping tool to set the keyword to \"Tesla Model\" and scrape comments from YouTube videos. By default, we scrape 20 comments per video and collect over 1700 comments in total.\n",
    "\n",
    "Finally, we convert the results to a csv file *Reddit&YouTube Comments.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX9ZqoojnxQT"
   },
   "source": [
    "# Extract Comments from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz0IptcW5Y5v"
   },
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jb-pL9vi5T5y"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from praw.models import MoreComments\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mluldph6n52P"
   },
   "source": [
    "## Search reddit and create a word cloud of 'Tesla model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AnVRhnxcoMoa"
   },
   "outputs": [],
   "source": [
    "# Set up a client for the Search API\n",
    "client_id=\"......\"\n",
    "client_secret=\"......\"\n",
    "password=\"******\"\n",
    "user_agent=\"......\"\n",
    "username=\"......\"\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kK8nJTWYwWv"
   },
   "outputs": [],
   "source": [
    "# Retrieve from the Search API\n",
    "\n",
    "# Set keyword and search in all subreddits\n",
    "keyword = \"tesla model\"\n",
    "search = reddit.subreddit(\"all\").search(keyword)\n",
    "\n",
    "# Set a blank list to save serch results\n",
    "search_posts = []\n",
    "\n",
    "# Extrac features\n",
    "for submission in search:\n",
    "\n",
    "  search_comments = []      #blank list to save comments\n",
    "\n",
    "  for comment in submission.comments:     #Search comments in every submission\n",
    "    if isinstance(comment, MoreComments):\n",
    "        continue\n",
    "    search_comments.append(comment.body)\n",
    "\n",
    "  # Add submission information to search result\n",
    "  search_posts.append([submission.url, submission.title, submission.id, submission.author, datetime.fromtimestamp(submission.created_utc), submission.num_comments, np.transpose(search_comments)])\n",
    "\n",
    "# Set Dataframe to sotre search results, and save to .csv file.\n",
    "submissions = pd.DataFrame(search_posts, columns=['Url', 'Title', 'Title_id', 'Author', 'Sub_time', 'No. of comments', 'Comments'])\n",
    "submissions.to_csv(f\"comments_Reddit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHKssvds0eYg"
   },
   "source": [
    "# Extract Comments from Youtube\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLuv9eaR0yii"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an edge instance, and search on Youtube with key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an edge instance\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "edge_options.add_argument('lang=en-US')\n",
    "service = Service('edgedriver_win64/msedgedriver.exe')  \n",
    "driver = bor = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "# set up search on Youtube with key word.\n",
    "key_word = 'Tesla model'\n",
    "url = f\"https://www.youtube.com/results?search_query={key_word}\"\n",
    "bor.get(url)\n",
    "time.sleep(15)   # wait to ensure the page is fully loaded\n",
    "\n",
    "accept_buttons = driver.find_elements(By.CSS_SELECTOR, '.yt-spec-button-shape-next.yt-spec-button-shape-next--filled')\n",
    "\n",
    "# Look for the button \"Accept all\"\n",
    "accept_button = [b for b in accept_buttons if b.text == 'Accept all']\n",
    "\n",
    "# Cilck the \"Accept all\" button if it exists\n",
    "if accept_button:\n",
    "    accept_button[0].click()\n",
    "    time.sleep(5)  # wait to ensure the click is finished\n",
    "else:\n",
    "    print(\"The button not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searche videos on Youtube with keyword, and save the results in a list.\n",
    "\n",
    "def search(key_word,fetch_numbers = 100):   # set key words, target number and url    \n",
    "    url = f\"https://www.youtube.com/results?search_query={key_word}\"\n",
    "    bor.get(url)\n",
    "    print('scrolling ....',end='  ')\n",
    "    while True:  # scroll down to load more videos\n",
    "        bor.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        wait_time = random.randint(10, 15)  # wait for 15 to 30 seconds randomly, to avoid being detected as a scraper.\n",
    "        time.sleep(wait_time)\n",
    "        search_items = bor.find_elements(by='tag name',value='ytd-video-renderer')\n",
    "        print(len(search_items),end=' ')\n",
    "        if len(search_items) > fetch_numbers:\n",
    "            break\n",
    "\n",
    "    search_page = []  # save search results\n",
    "    for one_item in search_items:\n",
    "        title_div = one_item.find_element(by='id',value='video-title')\n",
    "        title_text = title_div.text\n",
    "        link = title_div.get_attribute('href')\n",
    "        meta_text = one_item.find_element(by='tag name',value='ytd-video-meta-block').text\n",
    "        item_data = dict(title=title_text,link=link,meta=meta_text)\n",
    "        search_page.append(item_data)\n",
    "    return search_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect comments of videos.\n",
    "# Scroll down to load more comments, until get target nunber of comments, or there is no new comments.\n",
    "\n",
    "def get_comments(url,fetch_comment_num = 30):\n",
    "    bor.get(url)\n",
    "    time.sleep(random.randint(5, 8))\n",
    "    last_num = -100\n",
    "    print(f'scrolling ....{url}',end='  ')\n",
    "    height = 362\n",
    "    delta = 1500\n",
    "    zero_delta_cnt = 0\n",
    "    \n",
    "    # Scroll the page to load more comments.\n",
    "    while True:\n",
    "        bor.execute_script(f\"window.scrollTo(0, {height});\")\n",
    "        height += delta\n",
    "        time.sleep(random.randint(3, 5))\n",
    "\n",
    "        # Find all comments on the page\n",
    "        comments = driver.find_elements(by='tag name',value='ytd-comment-thread-renderer')\n",
    "        cnt_comments = len(comments)\n",
    "        delta = cnt_comments-last_num\n",
    "\n",
    "        # If there is no more comment, increment the zero_delta counter\n",
    "        if delta ==0:\n",
    "            zero_delta_cnt += 1\n",
    "        print(cnt_comments,end=' ')\n",
    "\n",
    "        # If load enough comments, or there is no new commets loaded, break the loop.\n",
    "        if (cnt_comments > fetch_comment_num) or zero_delta_cnt>3:\n",
    "              break\n",
    "\n",
    "        # Update the last_num to the current number of comments\n",
    "        last_num = cnt_comments\n",
    "\n",
    "\n",
    "    print('')\n",
    "    \n",
    "    # List to store the extracted comment.\n",
    "    comments_data = []\n",
    "\n",
    "    # Loop through the loaded comments and extract the text of each comment\n",
    "    for one_comment in comments:\n",
    "        comment = one_comment.find_element(by='id',value='content-text').text\n",
    "        comment_item = dict(url=url,title=driver.title,comment=comment)\n",
    "        comments_data.append(comment_item)\n",
    "    return comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search\n",
    "search_items = search(key_word)\n",
    "print(\"===================\")\n",
    "comments_data = []\n",
    "for one_item in search_items:\n",
    "    link = one_item['link']\n",
    "    comments = get_comments(link)\n",
    "    comments_data.extend(comments)\n",
    "    \n",
    "# Convert the comments data and video items into a pandas DataFrame, and save to .csv files.\n",
    "df = pd.DataFrame(comments_data)\n",
    "df.to_csv(f\"comments_Youtbe.csv\")\n",
    "\n",
    "df_item = pd.DataFrame(search_items)\n",
    "df_item.to_csv(f\"{key_word}_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
